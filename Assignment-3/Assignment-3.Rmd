---
title: "Regression"
author: "Asher John"
date: "10/31/2021"
output:
  pdf_document: default
  html_document: default
---
## EX. 6.22
 A. It is linear
 
 B. IT is not linear and cannot be expressed in the form in 6.5.
 
 C. It is linear.
 
 D. It is not linear and cannot be expressd on the form in 6.5.
 
 E. It is linear. 

## EX. 6.31. 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
library(readxl)
library(ggplot2)
library(dplyr)
library(broom)
library(ggpubr)
library(tidyverse)
library(coefplot)

mydata1 <- read_xlsx("Appendix.xlsx")

mydata <- mydata1[, c(3,2,4,9,11)]

mydata <- mydata %>% rename(Y = Infection_Risk, X_1 = Age, X_2 = Routine_Culturing_Ratio, X_3 = Average_Daily_Census, X_4 = Available_facilities_Services)

Region1 <- mydata[1:28, ]

Region2 <- mydata[29:60, ]

Region3 <- mydata[61:97, ]

Region4 <- mydata[98:113, ]

```
Region-1
```{r}

model1 <- lm(Y ~ X_1 + X_2 + X_3 + X_4, data = Region1)

anov1 <- aov(Y ~ X_1 + X_2 + X_3 + X_4, data = Region1)

summary(model1)

```
Region-2

```{r}

model2 <- lm(Y ~ X_1 + X_2 + X_3 + X_4, data = Region2)

anov2 <- aov(Y ~ X_1 + X_2 + X_3 + X_4, data = Region2)

summary(model2)
```

Region-3
```{r}

model3 <- lm(Y ~ X_1 + X_2 + X_3 + X_4, data = Region3)

anov3 <- aov(Y ~ X_1 + X_2 + X_3 + X_4, data = Region3)

summary(model3)

```

Region-4
```{r}

model4 <- lm(Y ~ X_1 + X_2 + X_3 + X_4, data = Region4)

anov4 <- aov(Y ~ X_1 + X_2 + X_3 + X_4, data = Region4)

summary(model4)

```

EX. 6.31.A. Estimated Regression Functions:

Region-1: Y = -3.349+0.116X_1+0.058X_2+0.002X_3+0.007X_4 

Region-2: Y= 2.291+0.004X_1+0.058X_2+0.001X_3+0.015X_4

Region-3: Y= -0.144+0.030X_1+0.102X_2+0.004X_3+0.008X_4

Region-4: Y= 1.566+0.035X_1+0.040X_2-0.0006X_3+0.012X_4


6.31.B:

Estimated regression functions are not the same for any of the four regions. All the four regions have different intercepts and different estimated parameters. I asume that these differences are due to the normal variation in the data. One reason that can be attributed to the differences in estimated regression functions could be that sample size is very small for each region. Another reason for differences is the factors that have not been taken nto consideration in the model. 

6.31.C

```{r}
summary(anov1)
summary(anov2)
summary(anov3)
summary(anov4)
```

   Region 1: R-squared:  0.4613, MSE: 1.022
   
   Region 2: R-squared:  0.4115, MSE: 1.212 
   
   Region 3: R-squared:  0.6088, MSE: 0.937 
   
   Region 4: R-squared:  0.0896, MSE: 0.9538

All the above values are different this is also due to the smaller size of the samples from the different regions. Another reason for differences is the factors that have not been taken nto consideration in the model. 

 
```{r}
coefplot(model1)
coefplot(model2)
coefplot(model3)
coefplot(model4)

```

EX. 6.31.D.
Box Plots show that the residuals are normally distributed around the mean.


# EX. 7.4
```{r}
data6.9 <- read_excel("Data_6.9.xlsx")
summary(data6.9)
fit <- lm(Y ~ X_1 + X_2 + X_3, data = data6.9)

round(summary(fit)$coef, 4)

anova(fit)

fit.aov <- anova(fit)
tab <- as.table(cbind(
  'SS' = c("SSR(x1, x2, x3)" = sum(fit.aov[1:3, 2]),
         "SSR(x1)"           = fit.aov[1, 2],
         "SSR(x3|x1)"        = fit.aov[3, 2],
         "SSR(x2|x1, x3)"    = fit.aov[2, 2],
         "SSE"               = fit.aov[4, 2],
         "Total"             = sum(fit.aov[, 2])),

  'Df' = c(                    sum(fit.aov[1:3, 1]),
                               fit.aov[1, 1],
                               fit.aov[3, 1],
                               fit.aov[2, 1],
                               fit.aov[4, 1],
                               sum(fit.aov$Df)),

  'MS' = c(                    sum(fit.aov[1:3, 2]) / sum(fit.aov[1:3, 1]),
                               fit.aov[1, 3],
                               fit.aov[3, 3],
                               fit.aov[2, 3],
                               fit.aov[4, 3],
                               NA)
))

round(tab, 2)
```
7.4-B. 

The answer is based on 7.2 in the textbook. 

Yes, the X_2 can be dropped from the model given X_1 and X_3 are retained. F value is obtained from the anova table above by (MSR(X2|X1,X3))/MSE; F = 5726/20532 = 0.279. To check if the null holds test was conducted. For a = .05, we require F(.95; 1, 48) = 4.043. Since F* = 0.279 < 4.043, we conclude Ho, and X_2 can be dropped. The P_value for the test is 0.6 which is way bigger than a = 0.05 and is not significant, hence the null holds and X_2 can be dropped. 

7.4-C. 

Yes, SSR(X1) +SSR(X2 given X1,) is equal SSR(X2) +SSR(X1 given X2) here. This will always be the case (refernce 7.8 & 7.9, page: 260 in the textbook).


# EX. 7.17
```{r}
scale1 <- data.frame(scale(data6.9))

model_scale <- lm(Y ~ X_1 + X_2 + X_3, data = scale1)

summary(model_scale)

anova_scale1 <- anova(lm(Y ~ X_1 + X_2 + X_3, data = scale1))

anova_scale1

round(summary(model_scale)$coef, 3)

cor(scale1)
```
 7.17.A.      

Y= 0.175X_1 - 0.046X_2 + 0.808X_3 

 7.17.B.
 
This table shows relationship between all predictor variables and the respnse varoable and it is meaniningful as it shows that X-3 has the strongest correlation with the response variable.       

 7.17.C. 
 
For transforming back to the original equation we use the following, these values were calculated on excel:

          b1 = (sY/s_1)b*_1 = (249/55274.64)*0.175 = 0.0008
          
          b2 = (sY/s_2)b*_2 = (249/0.88)*-0.046 = -13.02
          
          b3 = (sY/s_3)b*_3 = (249/0.323)*0.808 = 622.89
          
  For b_0 the formula on p. 277 from the textbook (last line is used) cannot be typed here!
  
          b_0 = 4363-.0008*302693 + 13.02*7.371 - 622.89*0.1154 = 4145

 The re-transformed equation will be:
 
 Y = 4145 + 0.008X_1 - 13.02X_2 + 622.89X_3. 
 
 This equation is pretty same as the one in 7.4, there are some differences due to manual calculations. The original was 
 
 Y =  4149.8872 + 0.008X_1 - -13.1660X_2 +  623.5545X_3. 



# EX. 7.38

 Preparing the data

```{r}
library(readxl)
mydata2 <- read_excel("Book2.xlsx")
mydata2

df.2 <- mydata2[, c(1,2,3,4,9,10,11)]

df.2

df.2 <- df.2 %>% rename(Y = "Lengh of stay", X_1 = "Age", 
                        X_2 = "Infection Risk", 
                        X_3 = "Routine Culturing Ratio", X_4 = "Average Daily Census", X_5 = "Number of Nurses", X_6 = "Available facilities and Services")

df.2

model7.38 <- lm(Y ~ X_1 + X_2 + X_3 + X_4, X_5, X_6, data = df.2)

anova_1 <- anova(lm(Y ~ X_1 + X_2 + X_3 + X_4, X_5, X_6, data = df.2))
anova_1

```

 7.38 A
```{r}
anova_2 <- anova(lm(Y ~ X_1 + X_2 + X_3, data = df.2))

anova_2

anova_2[3,2]/anova(lm(Y ~ X_1 + X_2, data = df.2)) [3,2]
```

```{r}
anova_3 <- anova(lm(Y ~ X_1 + X_2 + X_4, data = df.2))
anova_3
anova_3[3,2]/anova(lm(Y ~ X_1 + X_2, data = df.2)) [3,2]
```

```{r}
anova_4 <- anova(lm(Y ~ X_1 + X_2 + X_5, data = df.2))
anova_4
anova_4[3,2]/anova(lm(Y ~ X_1 + X_2, data = df.2)) [3,2]
```

```{r}
anova_5 <- anova(lm(Y ~ X_1 + X_2 + X_6, data = df.2))
anova_5
anova_5[3,2]/anova(lm(Y ~ X_1 + X_2, data = df.2)) [3,2]
```
7.38 A

 coefficients of partial determination are as following:

 X_3|X_1,X_2 = 0.01167293, extra sum of squares = 3.248

 X_4|X_1,X_2 = 0.1362033,  extra sum of squares = 37.899

 X_5|X_1,X_2 = 0.03736635, extra sum of squares = 10.397

 X_6|X_1,X_2 = 0.03638879, extra sum of squares = 10.125
 

7.38-B.

As we can see from the data above X_4 is the best predictor as it has the most part in  explianing the variation in Y given the X_1 and X-2 are already there. It also has the larger extra sum of squares (37.899) as compared to others.

 7.38-C.

The F-Value for X_4 is 17.187 and according to the test mentioned in chapter 7.3 "To test a single Ho:βk = 0 use “partial”F-test via F_K* = MSR(Xk|X1‚…‚Xk–1‚Xk+1‚…‚Xp–1)/MSE with Fk* ~ F(1,n–p)". The test was deployed and 17.87 > f(1, n-p) = 17.87 > f(1, 109) = 17.87 > 3.93. 
 The F_value is quite bigger than the critical f-value hence the alternative holds and  Ho:βk = 0 (null) is rejected. This means that X-4 is helpful in the regression model when X I and X2 are included in the model. The f-test statistics are not large for other variables, the F_value for X_3 is smaller than the critical value and X_5 and X_6 are just little bigger than the critical value for f @ f(1, n-p). 





